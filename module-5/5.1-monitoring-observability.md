# Module 5.1: Monitoring and Observability

## Progress Software - Container Training Series

---

## Overview

Effective monitoring and observability are critical for running containerized .NET applications in production. This module covers Azure Monitor, Container Insights, Application Insights, Log Analytics (KQL queries), alerting, and distributed tracing with OpenTelemetry.

---

## Learning Objectives

By the end of this lab, you will be able to:
- Enable and configure Azure Monitor Container Insights for AKS
- Integrate Application Insights into .NET containerized applications
- Write KQL queries to analyze container logs and metrics
- Create alerts and dashboards for proactive monitoring
- Implement Prometheus and Grafana for advanced metrics
- Configure distributed tracing with OpenTelemetry

---

## Prerequisites

- Completed Modules 1-4
- Azure CLI installed and configured
- An active AKS cluster (from Module 4)
- kubectl configured to access your cluster
- Azure Container Apps or App Service with containers deployed
- Basic understanding of .NET applications
- A .NET containerized application ready for instrumentation

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    Azure Monitor                             │
│  ┌──────────────────┐  ┌──────────────────┐                │
│  │ Container Insights│  │ Application      │                │
│  │                   │  │ Insights         │                │
│  └──────────────────┘  └──────────────────┘                │
│           │                      │                           │
│           └──────────┬───────────┘                           │
│                      │                                       │
│           ┌──────────▼──────────┐                           │
│           │ Log Analytics       │                           │
│           │ Workspace           │                           │
│           └─────────────────────┘                           │
└─────────────────────────────────────────────────────────────┘
                       │
        ┌──────────────┼──────────────┐
        │              │               │
   ┌────▼────┐   ┌────▼────┐   ┌─────▼─────┐
   │   AKS   │   │   ACA   │   │ App Svc   │
   │ Cluster │   │         │   │ Container │
   └─────────┘   └─────────┘   └───────────┘
```

---

## Lab 1: Enable Container Insights for AKS

### Overview
Container Insights provides comprehensive monitoring for AKS clusters, including node and pod metrics, logs, and performance data.

### Step 1: Create Log Analytics Workspace

```bash
# Set variables
export RESOURCE_GROUP="rg-containers-prod"
export LOCATION="eastus"
export LOG_WORKSPACE="law-containers-monitor"
export AKS_CLUSTER="aks-prod-cluster"

# Create Log Analytics workspace
az monitor log-analytics workspace create \
  --resource-group $RESOURCE_GROUP \
  --workspace-name $LOG_WORKSPACE \
  --location $LOCATION

# Get workspace ID
export WORKSPACE_ID=$(az monitor log-analytics workspace show \
  --resource-group $RESOURCE_GROUP \
  --workspace-name $LOG_WORKSPACE \
  --query id -o tsv)

echo "Workspace ID: $WORKSPACE_ID"
```

**Expected Output:**
```json
{
  "createdDate": "2026-01-29T10:15:30.123456Z",
  "customerId": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "id": "/subscriptions/.../resourceGroups/rg-containers-prod/providers/Microsoft.OperationalInsights/workspaces/law-containers-monitor",
  "location": "eastus",
  "name": "law-containers-monitor",
  "provisioningState": "Succeeded"
}
```

### Step 2: Enable Container Insights on AKS

```bash
# Enable monitoring addon for AKS
az aks enable-addons \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --addons monitoring \
  --workspace-resource-id $WORKSPACE_ID

# Verify the addon is enabled
az aks show \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --query "addonProfiles.omsagent"
```

**Expected Output:**
```json
{
  "config": {
    "logAnalyticsWorkspaceResourceID": "/subscriptions/.../workspaces/law-containers-monitor"
  },
  "enabled": true,
  "identity": null
}
```

### Step 3: Verify Container Insights Components

```bash
# Check if omsagent pods are running
kubectl get pods -n kube-system | grep omsagent

# Expected output:
# omsagent-5d7f8                    1/1     Running   0          2m15s
# omsagent-rs-7b9c5d4f8b-xk2ln      1/1     Running   0          2m15s
# omsagent-t9w4m                    1/1     Running   0          2m15s

# View omsagent configuration
kubectl get configmap -n kube-system | grep container-azm

# Check daemonset status
kubectl get daemonset -n kube-system omsagent
```

### Step 4: Access Container Insights

```bash
# Open Azure Portal to Container Insights
echo "Navigate to: Azure Portal > AKS Cluster > Monitoring > Insights"

# Or get direct link
az aks show \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --query id -o tsv | \
  xargs -I {} echo "https://portal.azure.com/#@/resource{}/containerInsights"
```

### What You'll See in Container Insights:
- **Cluster view**: Overall cluster health and resource utilization
- **Nodes view**: CPU, memory, and disk metrics per node
- **Controllers view**: Deployment, ReplicaSet, StatefulSet metrics
- **Containers view**: Individual container performance
- **Logs**: Integrated log search

---

## Lab 2: Configure Application Insights for .NET Containers

### Overview
Application Insights provides deep application performance monitoring (APM) for .NET applications running in containers.

### Step 1: Create Application Insights Resource

```bash
# Set variables
export APP_INSIGHTS_NAME="appi-dotnet-containers"

# Create Application Insights (requires Application Insights extension)
az extension add --name application-insights

az monitor app-insights component create \
  --app $APP_INSIGHTS_NAME \
  --location $LOCATION \
  --resource-group $RESOURCE_GROUP \
  --workspace $WORKSPACE_ID

# Get instrumentation key and connection string
export APPINSIGHTS_INSTRUMENTATIONKEY=$(az monitor app-insights component show \
  --app $APP_INSIGHTS_NAME \
  --resource-group $RESOURCE_GROUP \
  --query instrumentationKey -o tsv)

export APPINSIGHTS_CONNECTION_STRING=$(az monitor app-insights component show \
  --app $APP_INSIGHTS_NAME \
  --resource-group $RESOURCE_GROUP \
  --query connectionString -o tsv)

echo "Instrumentation Key: $APPINSIGHTS_INSTRUMENTATIONKEY"
echo "Connection String: $APPINSIGHTS_CONNECTION_STRING"
```

### Step 2: Instrument .NET Application

Create or update your .NET application to include Application Insights:

**Program.cs** (ASP.NET Core 6+):
```csharp
using Microsoft.ApplicationInsights.AspNetCore.Extensions;
using Microsoft.ApplicationInsights.Extensibility;

var builder = WebApplication.CreateBuilder(args);

// Add Application Insights telemetry
builder.Services.AddApplicationInsightsTelemetry(options =>
{
    options.ConnectionString = builder.Configuration["ApplicationInsights:ConnectionString"];
    options.EnableAdaptiveSampling = true;
    options.EnableQuickPulseMetricStream = true;
});

// Configure telemetry
builder.Services.Configure<TelemetryConfiguration>(config =>
{
    config.SetAzureTokenCredential(new DefaultAzureCredential());
});

builder.Services.AddControllers();
builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

var app = builder.Build();

app.UseHttpsRedirection();
app.UseAuthorization();
app.MapControllers();

app.Run();
```

**Dockerfile** (include Application Insights SDK):
```dockerfile
FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /src
COPY ["MyApp.csproj", "./"]
RUN dotnet restore "MyApp.csproj"
COPY . .
RUN dotnet build "MyApp.csproj" -c Release -o /app/build

FROM build AS publish
RUN dotnet publish "MyApp.csproj" -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT ["dotnet", "MyApp.dll"]
```

**.csproj** (add package reference):
```xml
<ItemGroup>
  <PackageReference Include="Microsoft.ApplicationInsights.AspNetCore" Version="2.22.0" />
</ItemGroup>
```

### Step 3: Deploy with Application Insights Configuration

**Kubernetes Deployment with Secret:**
```bash
# Create Kubernetes secret for connection string
kubectl create secret generic app-insights-secret \
  --from-literal=connection-string="$APPINSIGHTS_CONNECTION_STRING" \
  -n default

# Create deployment with environment variable
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dotnet-app-monitored
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dotnet-app
  template:
    metadata:
      labels:
        app: dotnet-app
    spec:
      containers:
      - name: dotnet-app
        image: myacr.azurecr.io/dotnet-app:v1.0
        ports:
        - containerPort: 80
        env:
        - name: ApplicationInsights__ConnectionString
          valueFrom:
            secretKeyRef:
              name: app-insights-secret
              key: connection-string
        - name: ASPNETCORE_ENVIRONMENT
          value: "Production"
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: dotnet-app-service
spec:
  selector:
    app: dotnet-app
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
EOF
```

### Step 4: Verify Telemetry Flow

```bash
# Wait for deployment
kubectl rollout status deployment/dotnet-app-monitored

# Get service endpoint
export SERVICE_IP=$(kubectl get service dotnet-app-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# Generate some traffic
for i in {1..50}; do
  curl -s http://$SERVICE_IP/api/health > /dev/null
  echo "Request $i sent"
  sleep 1
done

# Check Application Insights (wait 2-3 minutes for data)
echo "Check telemetry at: https://portal.azure.com/#@/resource$WORKSPACE_ID/appInsights"
```

### Step 5: Custom Telemetry

Add custom tracking to your application:

**Custom Telemetry Example:**
```csharp
using Microsoft.ApplicationInsights;
using Microsoft.ApplicationInsights.DataContracts;

public class OrderController : ControllerBase
{
    private readonly TelemetryClient _telemetryClient;
    
    public OrderController(TelemetryClient telemetryClient)
    {
        _telemetryClient = telemetryClient;
    }
    
    [HttpPost]
    public async Task<IActionResult> CreateOrder([FromBody] Order order)
    {
        var stopwatch = System.Diagnostics.Stopwatch.StartNew();
        
        try
        {
            // Track custom event
            _telemetryClient.TrackEvent("OrderCreated", new Dictionary<string, string>
            {
                { "OrderId", order.Id.ToString() },
                { "CustomerId", order.CustomerId.ToString() },
                { "Amount", order.TotalAmount.ToString() }
            });
            
            // Business logic here
            await ProcessOrder(order);
            
            // Track custom metric
            _telemetryClient.TrackMetric("OrderProcessingTime", stopwatch.ElapsedMilliseconds);
            
            return Ok(new { success = true, orderId = order.Id });
        }
        catch (Exception ex)
        {
            // Track exception
            _telemetryClient.TrackException(ex, new Dictionary<string, string>
            {
                { "OrderId", order.Id.ToString() },
                { "Operation", "CreateOrder" }
            });
            
            return StatusCode(500, "Order processing failed");
        }
    }
}
```

---

## Lab 3: Create Custom Log Analytics Queries (KQL)

### Overview
Kusto Query Language (KQL) enables powerful log analysis across all your container telemetry.

### Step 1: Access Log Analytics

```bash
# Get workspace details
az monitor log-analytics workspace show \
  --resource-group $RESOURCE_GROUP \
  --workspace-name $LOG_WORKSPACE

# Navigate to Logs in Azure Portal
echo "Go to: Azure Portal > Log Analytics Workspaces > $LOG_WORKSPACE > Logs"
```

### Step 2: Basic Container Queries

**Query 1: List all container logs from last hour**
```kql
ContainerLog
| where TimeGenerated > ago(1h)
| project TimeGenerated, Computer, ContainerID, LogEntry
| order by TimeGenerated desc
| take 100
```

**Query 2: Find container errors**
```kql
ContainerLog
| where TimeGenerated > ago(24h)
| where LogEntry contains "error" or LogEntry contains "exception" or LogEntry contains "fail"
| project TimeGenerated, Computer, ContainerID, LogEntrySource, LogEntry
| order by TimeGenerated desc
```

**Query 3: Container restart events**
```kql
KubePodInventory
| where TimeGenerated > ago(7d)
| where PodStatus == "Failed" or ContainerRestartCount > 0
| project TimeGenerated, Namespace, Name, ContainerRestartCount, PodStatus, Computer
| order by ContainerRestartCount desc
```

### Step 3: Performance Queries

**Query 4: CPU usage by container**
```kql
Perf
| where TimeGenerated > ago(1h)
| where ObjectName == "K8SContainer"
| where CounterName == "cpuUsageNanoCores"
| summarize AvgCPU = avg(CounterValue), MaxCPU = max(CounterValue) by InstanceName, bin(TimeGenerated, 5m)
| order by AvgCPU desc
```

**Query 5: Memory usage by pod**
```kql
Perf
| where TimeGenerated > ago(1h)
| where ObjectName == "K8SContainer"
| where CounterName == "memoryWorkingSetBytes"
| extend MemoryMB = CounterValue / 1024 / 1024
| summarize AvgMemoryMB = avg(MemoryMB), MaxMemoryMB = max(MemoryMB) by InstanceName, bin(TimeGenerated, 5m)
| order by MaxMemoryMB desc
```

**Query 6: Network traffic analysis**
```kql
Perf
| where TimeGenerated > ago(1h)
| where ObjectName == "K8SContainer"
| where CounterName in ("networkRxBytes", "networkTxBytes")
| extend TrafficMB = CounterValue / 1024 / 1024
| summarize TotalTrafficMB = sum(TrafficMB) by InstanceName, CounterName, bin(TimeGenerated, 5m)
| order by TotalTrafficMB desc
```

### Step 4: Application Insights Queries

**Query 7: Request performance**
```kql
requests
| where timestamp > ago(24h)
| summarize 
    RequestCount = count(),
    AvgDuration = avg(duration),
    P50 = percentile(duration, 50),
    P95 = percentile(duration, 95),
    P99 = percentile(duration, 99)
    by name, resultCode
| order by RequestCount desc
```

**Query 8: Failed requests**
```kql
requests
| where timestamp > ago(24h)
| where success == false
| project timestamp, name, url, resultCode, duration, customDimensions
| order by timestamp desc
```

**Query 9: Exception analysis**
```kql
exceptions
| where timestamp > ago(24h)
| summarize ExceptionCount = count() by type, outerMessage, cloud_RoleName
| order by ExceptionCount desc
```

**Query 10: Custom events tracking**
```kql
customEvents
| where timestamp > ago(24h)
| where name == "OrderCreated"
| extend OrderId = tostring(customDimensions.OrderId), Amount = todouble(customDimensions.Amount)
| summarize OrderCount = count(), TotalAmount = sum(Amount) by bin(timestamp, 1h)
| order by timestamp desc
```

### Step 5: Advanced Cross-Resource Queries

**Query 11: Correlate container logs with application telemetry**
```kql
let containerErrors = ContainerLog
| where TimeGenerated > ago(1h)
| where LogEntry contains "error"
| project TimeGenerated, ContainerID, LogEntry;
let appExceptions = exceptions
| where timestamp > ago(1h)
| project timestamp, type, outerMessage, cloud_RoleInstance;
containerErrors
| join kind=inner (appExceptions) on $left.TimeGenerated == $right.timestamp
| project TimeGenerated, ContainerID, LogEntry, type, outerMessage
```

**Query 12: Pod lifecycle and performance correlation**
```kql
KubePodInventory
| where TimeGenerated > ago(24h)
| where Name contains "dotnet-app"
| join kind=inner (
    Perf
    | where ObjectName == "K8SContainer"
    | where CounterName == "cpuUsageNanoCores"
) on Computer, $left.ContainerName == $right.InstanceName
| project TimeGenerated, Name, Namespace, PodStatus, CPUUsage = CounterValue
| summarize AvgCPU = avg(CPUUsage) by Name, PodStatus, bin(TimeGenerated, 10m)
```

### Step 6: Save Queries

```bash
# Save query via Azure CLI
az monitor log-analytics workspace saved-search create \
  --resource-group $RESOURCE_GROUP \
  --workspace-name $LOG_WORKSPACE \
  --name "ContainerErrors" \
  --category "Container Monitoring" \
  --query "ContainerLog | where LogEntry contains 'error' | project TimeGenerated, ContainerID, LogEntry" \
  --display-name "Container Error Logs"
```

---

## Lab 4: Set Up Alerts and Dashboards

### Overview
Proactive alerting helps you catch issues before they impact users.

### Step 1: Create Metric Alerts

**Alert 1: High CPU usage**
```bash
# Create alert for high CPU usage
az monitor metrics alert create \
  --name "AKS-HighCPU-Alert" \
  --resource-group $RESOURCE_GROUP \
  --scopes $(az aks show --resource-group $RESOURCE_GROUP --name $AKS_CLUSTER --query id -o tsv) \
  --condition "avg node_cpu_usage_percentage > 80" \
  --window-size 5m \
  --evaluation-frequency 1m \
  --description "Alert when AKS node CPU exceeds 80%" \
  --severity 2
```

**Alert 2: High memory usage**
```bash
az monitor metrics alert create \
  --name "AKS-HighMemory-Alert" \
  --resource-group $RESOURCE_GROUP \
  --scopes $(az aks show --resource-group $RESOURCE_GROUP --name $AKS_CLUSTER --query id -o tsv) \
  --condition "avg node_memory_working_set_percentage > 85" \
  --window-size 5m \
  --evaluation-frequency 1m \
  --description "Alert when AKS node memory exceeds 85%" \
  --severity 2
```

### Step 2: Create Log Query Alerts

**Alert 3: Container restart alert**
```bash
# Create action group first
az monitor action-group create \
  --name "ContainerOps-Team" \
  --resource-group $RESOURCE_GROUP \
  --short-name "ContOps" \
  --email-receiver name="OpsTeam" email="ops@progresssoftware.com"

# Create log alert
az monitor scheduled-query create \
  --name "Container-Restart-Alert" \
  --resource-group $RESOURCE_GROUP \
  --scopes $WORKSPACE_ID \
  --condition "count > 5" \
  --condition-query "KubePodInventory | where ContainerRestartCount > 0 | summarize AggregatedValue = count() by Computer, ContainerName" \
  --description "Alert when containers restart more than 5 times" \
  --evaluation-frequency 5m \
  --window-size 15m \
  --severity 2 \
  --action-groups $(az monitor action-group show --name "ContainerOps-Team" --resource-group $RESOURCE_GROUP --query id -o tsv)
```

**Alert 4: Application error rate**
```bash
az monitor scheduled-query create \
  --name "App-HighErrorRate-Alert" \
  --resource-group $RESOURCE_GROUP \
  --scopes $(az monitor app-insights component show --app $APP_INSIGHTS_NAME --resource-group $RESOURCE_GROUP --query id -o tsv) \
  --condition "count > 10" \
  --condition-query "requests | where success == false | summarize AggregatedValue = count()" \
  --description "Alert when error rate exceeds 10 per 5 minutes" \
  --evaluation-frequency 5m \
  --window-size 5m \
  --severity 1 \
  --action-groups $(az monitor action-group show --name "ContainerOps-Team" --resource-group $RESOURCE_GROUP --query id -o tsv)
```

### Step 3: Create Azure Dashboard

**Create dashboard JSON:**
```bash
cat > dashboard.json <<'EOF'
{
  "properties": {
    "lenses": [
      {
        "order": 0,
        "parts": [
          {
            "position": {"x": 0, "y": 0, "colSpan": 6, "rowSpan": 4},
            "metadata": {
              "type": "Extension/HubsExtension/PartType/MonitorChartPart",
              "settings": {
                "content": {
                  "options": {
                    "chart": {
                      "metrics": [
                        {
                          "resourceMetadata": {"id": "AKS_CLUSTER_ID"},
                          "name": "node_cpu_usage_percentage",
                          "aggregationType": 4,
                          "namespace": "microsoft.containerservice/managedclusters",
                          "metricVisualization": {
                            "displayName": "CPU Usage %"
                          }
                        }
                      ],
                      "title": "AKS Node CPU Usage",
                      "titleKind": 1,
                      "visualization": {
                        "chartType": 2
                      }
                    }
                  }
                }
              }
            }
          }
        ]
      }
    ]
  }
}
EOF

# Deploy dashboard
az portal dashboard create \
  --name "Container-Monitoring-Dashboard" \
  --resource-group $RESOURCE_GROUP \
  --input-path dashboard.json \
  --location $LOCATION
```

### Step 4: Create Workbook for Container Insights

```bash
# Navigate to Azure Portal
echo "Creating custom workbook..."
echo "Go to: Azure Portal > Azure Workbooks > + New"
echo "Use the following template for comprehensive container monitoring"
```

**Workbook KQL Queries to Include:**

1. **Container Health Overview**
```kql
KubePodInventory
| where TimeGenerated > ago(15m)
| summarize 
    TotalPods = dcount(Name),
    RunningPods = dcountif(Name, PodStatus == "Running"),
    FailedPods = dcountif(Name, PodStatus == "Failed"),
    PendingPods = dcountif(Name, PodStatus == "Pending")
| extend HealthScore = (RunningPods * 100.0) / TotalPods
```

2. **Resource Utilization Summary**
```kql
Perf
| where TimeGenerated > ago(1h)
| where ObjectName == "K8SContainer"
| where CounterName in ("cpuUsageNanoCores", "memoryWorkingSetBytes")
| summarize Value = avg(CounterValue) by CounterName
| extend 
    Metric = case(
        CounterName == "cpuUsageNanoCores", "CPU",
        CounterName == "memoryWorkingSetBytes", "Memory",
        "Unknown"
    ),
    DisplayValue = case(
        CounterName == "memoryWorkingSetBytes", Value / 1024 / 1024 / 1024,
        Value
    ),
    Unit = case(
        CounterName == "memoryWorkingSetBytes", "GB",
        "nanoCores"
    )
| project Metric, DisplayValue, Unit
```

---

## Lab 5: Prometheus and Grafana Integration (Optional)

### Overview
For advanced metrics and visualization, integrate Prometheus and Grafana with your AKS cluster.

### Step 1: Install Prometheus via Helm

```bash
# Add Prometheus Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# Create namespace
kubectl create namespace monitoring

# Install Prometheus
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
  --set grafana.enabled=true \
  --set grafana.adminPassword='P@ssw0rd123!' \
  --set prometheus.prometheusSpec.retention=7d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=20Gi

# Wait for all pods to be ready
kubectl wait --for=condition=ready pod -l "release=prometheus" -n monitoring --timeout=300s

# Check deployment
kubectl get pods -n monitoring
```

**Expected Output:**
```
NAME                                                   READY   STATUS    RESTARTS   AGE
alertmanager-prometheus-kube-prometheus-alertmanager-0 2/2     Running   0          2m
prometheus-grafana-7d5b8f9f8d-x7k2m                    3/3     Running   0          2m
prometheus-kube-prometheus-operator-6f8c8d8b7d-9nm4l   1/1     Running   0          2m
prometheus-kube-state-metrics-5d9f8b9c8d-vx2nm         1/1     Running   0          2m
prometheus-prometheus-kube-prometheus-prometheus-0     2/2     Running   0          2m
prometheus-prometheus-node-exporter-xxxxx              1/1     Running   0          2m
```

### Step 2: Access Grafana Dashboard

```bash
# Port-forward Grafana service
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 &

# Get Grafana admin password
export GRAFANA_PASSWORD=$(kubectl get secret -n monitoring prometheus-grafana \
  -o jsonpath="{.data.admin-password}" | base64 --decode)

echo "Grafana URL: http://localhost:3000"
echo "Username: admin"
echo "Password: $GRAFANA_PASSWORD"
```

### Step 3: Create Custom ServiceMonitor for .NET App

```bash
cat <<EOF | kubectl apply -f -
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dotnet-app-monitor
  namespace: default
  labels:
    app: dotnet-app
spec:
  selector:
    matchLabels:
      app: dotnet-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
EOF
```

### Step 4: Instrument .NET App for Prometheus

**Add Prometheus metrics to your .NET app:**

```csharp
// Install: dotnet add package prometheus-net.AspNetCore

using Prometheus;

var builder = WebApplication.CreateBuilder(args);

builder.Services.AddControllers();

var app = builder.Build();

// Enable metrics endpoint
app.UseMetricServer(); // Exposes /metrics
app.UseHttpMetrics();  // Captures HTTP metrics

app.UseAuthorization();
app.MapControllers();

app.Run();
```

**Update Dockerfile to expose metrics port:**
```dockerfile
EXPOSE 80
EXPOSE 443
# Metrics port
EXPOSE 9090
```

**Update Kubernetes Service:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: dotnet-app-service
  labels:
    app: dotnet-app
spec:
  selector:
    app: dotnet-app
  ports:
  - name: http
    port: 80
    targetPort: 80
  - name: metrics
    port: 9090
    targetPort: 9090
  type: LoadBalancer
EOF
```

### Step 5: Import Grafana Dashboards

```bash
# Import pre-built Kubernetes dashboards
echo "Import the following dashboard IDs in Grafana:"
echo "  - 8588: Kubernetes Cluster Monitoring"
echo "  - 6417: Kubernetes Cluster Monitoring (Prometheus)"
echo "  - 315: Kubernetes cluster monitoring (via Prometheus)"
echo "  - 10000: Kubernetes / Views / Global"
```

---

## Lab 6: Distributed Tracing with OpenTelemetry

### Overview
OpenTelemetry provides standardized distributed tracing across microservices.

### Step 1: Install OpenTelemetry Packages

```bash
# In your .NET project directory
dotnet add package OpenTelemetry.Extensions.Hosting
dotnet add package OpenTelemetry.Instrumentation.AspNetCore
dotnet add package OpenTelemetry.Instrumentation.Http
dotnet add package OpenTelemetry.Instrumentation.SqlClient
dotnet add package OpenTelemetry.Exporter.Console
dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol
dotnet add package Azure.Monitor.OpenTelemetry.Exporter
```

### Step 2: Configure OpenTelemetry in .NET

**Program.cs:**
```csharp
using OpenTelemetry;
using OpenTelemetry.Resources;
using OpenTelemetry.Trace;
using Azure.Monitor.OpenTelemetry.Exporter;

var builder = WebApplication.CreateBuilder(args);

// Configure OpenTelemetry
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource => resource
        .AddService(serviceName: "dotnet-app",
                    serviceVersion: "1.0.0",
                    serviceInstanceId: Environment.MachineName))
    .WithTracing(tracing => tracing
        .AddAspNetCoreInstrumentation(options =>
        {
            options.RecordException = true;
            options.Filter = (httpContext) =>
            {
                // Don't trace health check endpoints
                return !httpContext.Request.Path.Value?.Contains("/health") ?? true;
            };
        })
        .AddHttpClientInstrumentation()
        .AddSqlClientInstrumentation(options =>
        {
            options.SetDbStatementForText = true;
            options.RecordException = true;
        })
        .AddConsoleExporter()
        .AddAzureMonitorTraceExporter(options =>
        {
            options.ConnectionString = builder.Configuration["ApplicationInsights:ConnectionString"];
        }));

var app = builder.Build();

app.MapControllers();
app.Run();
```

### Step 3: Add Custom Tracing

```csharp
using System.Diagnostics;

public class OrderService
{
    private static readonly ActivitySource ActivitySource = new ActivitySource("OrderService");
    
    public async Task<Order> ProcessOrder(Order order)
    {
        using var activity = ActivitySource.StartActivity("ProcessOrder");
        activity?.SetTag("order.id", order.Id);
        activity?.SetTag("customer.id", order.CustomerId);
        
        try
        {
            // Validate order
            using (var validateActivity = ActivitySource.StartActivity("ValidateOrder"))
            {
                await ValidateOrder(order);
                validateActivity?.SetTag("validation.result", "success");
            }
            
            // Calculate pricing
            using (var pricingActivity = ActivitySource.StartActivity("CalculatePricing"))
            {
                var price = await CalculatePrice(order);
                pricingActivity?.SetTag("price.amount", price);
            }
            
            // Save to database
            using (var dbActivity = ActivitySource.StartActivity("SaveToDatabase"))
            {
                await SaveOrder(order);
            }
            
            activity?.SetTag("order.status", "completed");
            return order;
        }
        catch (Exception ex)
        {
            activity?.SetStatus(ActivityStatusCode.Error, ex.Message);
            activity?.RecordException(ex);
            throw;
        }
    }
}
```

**Register ActivitySource in Program.cs:**
```csharp
.WithTracing(tracing => tracing
    .AddSource("OrderService")
    .AddAspNetCoreInstrumentation()
    // ... other configurations
)
```

### Step 4: Deploy and Verify Tracing

```bash
# Rebuild and push container
docker build -t myacr.azurecr.io/dotnet-app:v2.0-otel .
docker push myacr.azurecr.io/dotnet-app:v2.0-otel

# Update Kubernetes deployment
kubectl set image deployment/dotnet-app-monitored \
  dotnet-app=myacr.azurecr.io/dotnet-app:v2.0-otel

# Generate traffic
for i in {1..20}; do
  curl -X POST http://$SERVICE_IP/api/orders \
    -H "Content-Type: application/json" \
    -d '{"customerId": "123", "items": [{"id": 1, "qty": 2}]}'
  sleep 2
done

# View traces in Application Insights
echo "Check traces at: https://portal.azure.com > Application Insights > Transaction search"
```

### Step 5: Analyze Distributed Traces

In Application Insights, you'll see:
- **End-to-end transaction**: Complete request flow
- **Dependency calls**: HTTP, SQL, external APIs
- **Performance timeline**: Time spent in each operation
- **Failures and exceptions**: Highlighted in red

**Sample KQL for Trace Analysis:**
```kql
traces
| where timestamp > ago(1h)
| where customDimensions.["order.id"] == "12345"
| project timestamp, message, operation_Name, customDimensions
| order by timestamp asc
```

---

## Monitoring Best Practices

### 1. Define Key Metrics (Golden Signals)

Monitor the four golden signals for each service:
- **Latency**: Response time for requests
- **Traffic**: Request rate
- **Errors**: Failed requests rate
- **Saturation**: Resource utilization (CPU, memory, disk)

### 2. Implement Health Checks

```csharp
// Add health checks to your .NET app
builder.Services.AddHealthChecks()
    .AddCheck("self", () => HealthCheckResult.Healthy())
    .AddSqlServer(connectionString, name: "database")
    .AddAzureServiceBusQueue(connectionString, queueName, name: "servicebus");

app.MapHealthChecks("/health");
app.MapHealthChecks("/health/ready", new HealthCheckOptions
{
    Predicate = check => check.Tags.Contains("ready")
});
```

### 3. Set Appropriate Retention Policies

```bash
# Set Log Analytics retention (30-730 days)
az monitor log-analytics workspace update \
  --resource-group $RESOURCE_GROUP \
  --workspace-name $LOG_WORKSPACE \
  --retention-time 90

# Set different retention for specific tables
az monitor log-analytics workspace table update \
  --resource-group $RESOURCE_GROUP \
  --workspace-name $LOG_WORKSPACE \
  --name ContainerLog \
  --retention-time 30
```

### 4. Use Sampling for High-Volume Applications

```csharp
// Configure adaptive sampling in Application Insights
builder.Services.AddApplicationInsightsTelemetry(options =>
{
    options.EnableAdaptiveSampling = true;
});

builder.Services.Configure<TelemetryConfiguration>(config =>
{
    var builder = config.DefaultTelemetrySink.TelemetryProcessorChainBuilder;
    builder.UseAdaptiveSampling(maxTelemetryItemsPerSecond: 5);
    builder.Build();
});
```

### 5. Create SLIs and SLOs

Define Service Level Indicators and Objectives:

```kql
// SLI: 99% of requests should complete in < 500ms
requests
| where timestamp > ago(7d)
| summarize 
    Total = count(),
    Fast = countif(duration < 500),
    SLI = (countif(duration < 500) * 100.0) / count()
| extend SLO = 99.0, Status = iff(SLI >= 99.0, "✓ Meeting SLO", "✗ Below SLO")
```

### 6. Alert Fatigue Prevention

- Use **action groups** to route alerts appropriately
- Set **severity levels** correctly
- Implement **alert throttling** to avoid spam
- Use **smart groups** to consolidate related alerts
- Configure **auto-resolution** when appropriate

```bash
# Create action group with multiple channels
az monitor action-group create \
  --name "Critical-Alerts" \
  --resource-group $RESOURCE_GROUP \
  --short-name "CritAlrt" \
  --email-receiver name="OnCall" email="oncall@progresssoftware.com" \
  --sms-receiver name="OnCallSMS" country-code="1" phone-number="5555551234" \
  --webhook-receiver name="PagerDuty" service-uri="https://events.pagerduty.com/integration/xxxxx/enqueue"
```

### 7. Cost Optimization

- Use **basic logs** for high-volume, low-value data
- Implement **log filtering** at the source
- Set **appropriate retention periods**
- Use **data collection rules** to filter unnecessary data

```bash
# Create data collection rule to filter logs
az monitor data-collection rule create \
  --name "FilteredContainerLogs" \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION \
  --rule-file dcr-config.json
```

---

## Troubleshooting Monitoring Issues

### Issue 1: No Data Appearing in Container Insights

**Symptoms:** Container Insights shows no data or stale data

**Solutions:**
```bash
# Check omsagent status
kubectl get pods -n kube-system | grep omsagent
kubectl logs -n kube-system -l component=oms-agent --tail=100

# Verify configuration
kubectl get configmap -n kube-system container-azm-ms-agentconfig -o yaml

# Restart omsagent
kubectl rollout restart daemonset omsagent -n kube-system

# Check workspace connection
az aks show \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --query "addonProfiles.omsagent"
```

### Issue 2: Application Insights Not Receiving Telemetry

**Symptoms:** No telemetry data in Application Insights

**Solutions:**
```bash
# Verify connection string is set
kubectl get deployment dotnet-app-monitored -o yaml | grep -A 5 "ApplicationInsights"

# Check pod logs for errors
kubectl logs -l app=dotnet-app --tail=100 | grep -i "application insights\|telemetry"

# Test connection from pod
kubectl exec -it <pod-name> -- curl -v https://dc.services.visualstudio.com/v2/track

# Verify instrumentation key
az monitor app-insights component show \
  --app $APP_INSIGHTS_NAME \
  --resource-group $RESOURCE_GROUP \
  --query "instrumentationKey"
```

### Issue 3: High Log Analytics Costs

**Symptoms:** Unexpectedly high Azure Monitor costs

**Solutions:**
```bash
# Analyze data ingestion by table
# Run in Log Analytics:
```
```kql
Usage
| where TimeGenerated > ago(30d)
| where IsBillable == true
| summarize IngestedGB = sum(Quantity) / 1024 by DataType
| order by IngestedGB desc
```

```bash
# Identify top log sources
kubectl top pods --all-namespaces

# Reduce log verbosity
kubectl set env daemonset/omsagent -n kube-system \
  AZMON_LOG_COLLECTION_SETTINGS='{"stdout": {"enabled": true, "exclude_namespaces": ["kube-system", "gatekeeper-system"]}}'
```

---

## Summary

In this lab, you learned how to:
- ✅ Enable Container Insights for comprehensive AKS monitoring
- ✅ Integrate Application Insights into .NET containerized applications
- ✅ Write powerful KQL queries for log analysis
- ✅ Create proactive alerts and dashboards
- ✅ Deploy Prometheus and Grafana for advanced metrics
- ✅ Implement distributed tracing with OpenTelemetry
- ✅ Follow monitoring best practices for production workloads

---

## Additional Resources

- [Azure Monitor Documentation](https://docs.microsoft.com/azure/azure-monitor/)
- [Container Insights Overview](https://docs.microsoft.com/azure/azure-monitor/containers/container-insights-overview)
- [Application Insights for .NET](https://docs.microsoft.com/azure/azure-monitor/app/asp-net-core)
- [KQL Quick Reference](https://docs.microsoft.com/azure/data-explorer/kql-quick-reference)
- [OpenTelemetry .NET](https://opentelemetry.io/docs/instrumentation/net/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)

---

## Next Steps

Continue to [Module 5.2: Troubleshooting Techniques](5.2-troubleshooting-techniques.md) to learn how to diagnose and resolve common container issues.

---

**Progress Software - Empowering Developers with Container Excellence**
