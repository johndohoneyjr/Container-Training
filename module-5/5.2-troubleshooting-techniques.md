# Module 5.2: Troubleshooting Techniques

## Progress Software - Container Training Series

---

## Overview

Even with the best planning, container deployments can encounter issues. This module provides comprehensive troubleshooting techniques, common problem patterns, and solutions for containerized .NET applications running in Azure.

---

## Learning Objectives

By the end of this lab, you will be able to:
- Debug containers locally using Docker tools
- Troubleshoot remote containers in Azure (ACI, ACA, AKS)
- Diagnose and resolve image pull errors
- Fix networking and connectivity issues
- Identify and resolve resource constraint problems
- Optimize container performance
- Use kubectl effectively for AKS troubleshooting
- Analyze container logs for root cause analysis

---

## Prerequisites

- Completed Modules 1-4
- Docker Desktop installed
- Azure CLI installed and configured
- kubectl installed and configured
- An active AKS cluster
- Basic understanding of networking concepts
- Containers deployed to Azure services

---

## Common Container Issues Matrix

| Issue Category | Symptoms | Common Causes | Tools to Use |
|----------------|----------|---------------|--------------|
| **Startup Failures** | Container exits immediately | Missing dependencies, configuration errors | `docker logs`, `kubectl describe` |
| **Image Pull Errors** | `ImagePullBackOff` | Registry auth, network issues, wrong image name | `docker pull`, `kubectl events` |
| **Networking** | Can't reach services | Firewall, DNS, port misconfiguration | `curl`, `nslookup`, `kubectl exec` |
| **Performance** | Slow response times | Resource limits, inefficient code | `kubectl top`, `docker stats` |
| **Crashes** | Container restarts repeatedly | Memory leaks, unhandled exceptions | `kubectl logs --previous`, Application Insights |
| **Storage** | Out of disk space | Log accumulation, temp files | `docker system df`, `du -h` |

---

## Lab 1: Debug Containers Locally with Docker

### Overview
Local debugging is the fastest way to identify and fix container issues before deploying to Azure.

### Step 1: Basic Container Debugging

```bash
# Set up test environment
export RESOURCE_GROUP="rg-containers-troubleshoot"
export ACR_NAME="acrtroubleshoot$RANDOM"
export LOCATION="eastus"

# Create a sample .NET application with a bug
mkdir -p ~/container-debug-lab
cd ~/container-debug-lab

# Create a buggy ASP.NET Core app
cat > Program.cs <<'EOF'
var builder = WebApplication.CreateBuilder(args);
var app = builder.Build();

app.MapGet("/", () => "Hello World!");

app.MapGet("/bug", () => 
{
    // This will cause a null reference exception
    string value = null;
    return value.ToUpper();
});

app.MapGet("/memory-leak", () => 
{
    // Memory leak simulation
    var list = new List<byte[]>();
    for (int i = 0; i < 1000; i++)
    {
        list.Add(new byte[1024 * 1024]); // 1MB each
    }
    return "Memory allocated";
});

// Missing environment variable handling
var dbConnection = Environment.GetEnvironmentVariable("DB_CONNECTION");
Console.WriteLine($"Connecting to: {dbConnection}");

app.Run();
EOF

# Create project file
cat > DebugApp.csproj <<'EOF'
<Project Sdk="Microsoft.NET.Sdk.Web">
  <PropertyGroup>
    <TargetFramework>net8.0</TargetFramework>
    <Nullable>enable</Nullable>
  </PropertyGroup>
</Project>
EOF

# Create Dockerfile
cat > Dockerfile <<'EOF'
FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base
WORKDIR /app
EXPOSE 80

FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /src
COPY ["DebugApp.csproj", "./"]
RUN dotnet restore "DebugApp.csproj"
COPY . .
RUN dotnet build "DebugApp.csproj" -c Release -o /app/build

FROM build AS publish
RUN dotnet publish "DebugApp.csproj" -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT ["dotnet", "DebugApp.dll"]
EOF
```

### Step 2: Build and Run with Issues

```bash
# Build the image
docker build -t debug-app:v1 .

# Run and observe the issue
docker run -d --name debug-test -p 8080:80 debug-app:v1

# Check if container is running
docker ps -a

# Expected output: Container may be in "Exited" state or constantly restarting
```

**Expected Issue:**
```
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS                     PORTS     NAMES
a1b2c3d4e5f6   debug-app:v1    "dotnet DebugApp.dll"   5 seconds ago   Exited (139) 2 seconds ago           debug-test
```

### Step 3: Diagnose the Issue

```bash
# Check container logs
docker logs debug-test

# Expected output showing the error:
# Connecting to: 
# Unhandled exception. System.ArgumentNullException: Value cannot be null.

# Inspect container details
docker inspect debug-test

# Check exit code (139 typically means segfault/crash)
docker inspect debug-test --format='{{.State.ExitCode}}'

# View recent container events
docker events --since 5m

# Check resource usage (if container was running)
docker stats debug-test --no-stream
```

### Step 4: Interactive Debugging

```bash
# Run container interactively to debug
docker run -it --rm debug-app:v1 /bin/bash

# Inside container, check environment
echo $PATH
ls -la /app
dotnet --version

# Try to run the app manually
dotnet /app/DebugApp.dll

# Exit the container
exit

# Run with environment variable
docker run -d --name debug-test-fixed \
  -p 8080:80 \
  -e DB_CONNECTION="Server=localhost;Database=test" \
  debug-app:v1

# Verify it's running
docker ps | grep debug-test-fixed

# Test the endpoints
curl http://localhost:8080/
curl http://localhost:8080/bug  # This will still cause an error in the app

# Check logs for the exception
docker logs debug-test-fixed --tail 50
```

### Step 5: Debug Running Container

```bash
# Execute commands in running container
docker exec -it debug-test-fixed /bin/bash

# Inside container, check processes
ps aux

# Check network connectivity
apt-get update && apt-get install -y curl
curl http://localhost:80

# Check file system
df -h
du -sh /app/*

# Check environment variables
env | sort

# Exit container
exit
```

### Step 6: Use Docker Debug Mode

```bash
# Build with debug configuration
cat > Dockerfile.debug <<'EOF'
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS debug
WORKDIR /app
EXPOSE 80
EXPOSE 5001

COPY . .
RUN dotnet restore
RUN dotnet build -c Debug

ENTRYPOINT ["dotnet", "run", "--no-build", "-c", "Debug"]
EOF

# Build debug image
docker build -f Dockerfile.debug -t debug-app:debug .

# Run with volume mount for live editing
docker run -d --name debug-live \
  -p 8080:80 \
  -v $(pwd):/app \
  debug-app:debug

# View real-time logs
docker logs -f debug-live
```

### Step 7: Analyze Container Resource Usage

```bash
# Monitor resource usage in real-time
docker stats

# Expected output:
# CONTAINER ID   NAME              CPU %     MEM USAGE / LIMIT     MEM %     NET I/O         BLOCK I/O
# a1b2c3d4e5f6   debug-test-fixed  0.50%     45.2MiB / 7.775GiB   0.57%     1.2kB / 648B    0B / 0B

# Generate load and watch memory
for i in {1..10}; do
  curl http://localhost:8080/memory-leak &
done

# Watch memory grow
docker stats debug-test-fixed --no-stream

# Check Docker disk usage
docker system df

# Detailed view
docker system df -v
```

### Step 8: Clean Up Test Containers

```bash
# Stop and remove containers
docker stop debug-test-fixed debug-live
docker rm debug-test-fixed debug-live debug-test

# Remove test images
docker rmi debug-app:v1 debug-app:debug
```

---

## Lab 2: Remote Debugging in Azure (ACI, ACA, AKS)

### Overview
Debug containers running in Azure services using various remote debugging techniques.

### Step 1: Debugging Azure Container Instances (ACI)

```bash
# Create resource group
az group create --name $RESOURCE_GROUP --location $LOCATION

# Deploy a container with issues
az container create \
  --resource-group $RESOURCE_GROUP \
  --name aci-debug-test \
  --image mcr.microsoft.com/dotnet/samples:aspnetapp \
  --dns-name-label aci-debug-$RANDOM \
  --ports 80 \
  --environment-variables 'ASPNETCORE_ENVIRONMENT=Production' \
  --cpu 1 \
  --memory 1

# Check container status
az container show \
  --resource-group $RESOURCE_GROUP \
  --name aci-debug-test \
  --query "{Name:name, State:instanceView.state, IP:ipAddress.ip}" \
  --output table

# View container logs
az container logs \
  --resource-group $RESOURCE_GROUP \
  --name aci-debug-test \
  --follow

# Get container events
az container show \
  --resource-group $RESOURCE_GROUP \
  --name aci-debug-test \
  --query "instanceView.events[]" \
  --output table

# Execute command in ACI container
az container exec \
  --resource-group $RESOURCE_GROUP \
  --name aci-debug-test \
  --exec-command "/bin/bash"

# Inside the container
ps aux
df -h
env | sort
curl http://localhost:80
exit

# Attach to container output
az container attach \
  --resource-group $RESOURCE_GROUP \
  --name aci-debug-test
```

### Step 2: Debugging Azure Container Apps (ACA)

```bash
# Set variables
export ACA_ENV="aca-env-debug"
export ACA_APP="aca-debug-app"

# Create Container Apps environment
az containerapp env create \
  --name $ACA_ENV \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION

# Deploy application
az containerapp create \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --environment $ACA_ENV \
  --image mcr.microsoft.com/dotnet/samples:aspnetapp \
  --target-port 80 \
  --ingress 'external' \
  --cpu 0.5 \
  --memory 1.0Gi \
  --min-replicas 1 \
  --max-replicas 3

# Get application URL
export ACA_URL=$(az containerapp show \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --query "properties.configuration.ingress.fqdn" -o tsv)

echo "Application URL: https://$ACA_URL"

# View logs (requires log analytics)
az containerapp logs show \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --follow

# Get replica details
az containerapp replica list \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --output table

# Execute command in a replica
export REPLICA_NAME=$(az containerapp replica list \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --query "[0].name" -o tsv)

az containerapp exec \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --replica $REPLICA_NAME \
  --command /bin/bash

# Get revision history
az containerapp revision list \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --output table

# Check revision details
az containerapp revision show \
  --name $ACA_APP \
  --resource-group $RESOURCE_GROUP \
  --revision $(az containerapp revision list --name $ACA_APP --resource-group $RESOURCE_GROUP --query "[0].name" -o tsv)
```

### Step 3: Debugging AKS Containers

```bash
# Set variables
export AKS_CLUSTER="aks-debug-cluster"
export AKS_RG="rg-aks-debug"

# Ensure kubectl is configured
az aks get-credentials \
  --resource-group $AKS_RG \
  --name $AKS_CLUSTER \
  --overwrite-existing

# Deploy test application
kubectl create deployment nginx-debug --image=nginx:latest
kubectl expose deployment nginx-debug --port=80 --type=LoadBalancer

# Check pod status
kubectl get pods -l app=nginx-debug

# Describe pod for detailed info
export POD_NAME=$(kubectl get pods -l app=nginx-debug -o jsonpath='{.items[0].metadata.name}')
kubectl describe pod $POD_NAME

# View pod logs
kubectl logs $POD_NAME

# Follow logs in real-time
kubectl logs -f $POD_NAME

# View previous container logs (if restarted)
kubectl logs $POD_NAME --previous

# Execute command in pod
kubectl exec -it $POD_NAME -- /bin/bash

# Inside pod
ps aux
netstat -tulpn
df -h
cat /etc/resolv.conf
exit

# Port-forward for local testing
kubectl port-forward $POD_NAME 8080:80 &

curl http://localhost:8080

# Stop port-forward
pkill -f "kubectl port-forward"

# Check pod events
kubectl get events --field-selector involvedObject.name=$POD_NAME --sort-by='.lastTimestamp'

# Check pod resource usage
kubectl top pod $POD_NAME

# Get pod YAML for inspection
kubectl get pod $POD_NAME -o yaml

# Copy files from pod
kubectl cp $POD_NAME:/var/log/nginx/access.log ./access.log

# Copy files to pod
echo "test" > test.txt
kubectl cp test.txt $POD_NAME:/tmp/test.txt
```

### Step 4: Debug Pod Networking

```bash
# Create a network debugging pod
kubectl run netdebug --image=nicolaka/netshoot --rm -it -- /bin/bash

# Inside netdebug pod, test connectivity
nslookup kubernetes.default
nslookup nginx-debug
curl http://nginx-debug

# Test external connectivity
ping -c 3 google.com
curl https://www.google.com

# Check DNS resolution
cat /etc/resolv.conf

# Trace route
traceroute google.com

# Check open ports
nmap nginx-debug

exit

# Debug from specific pod
kubectl exec -it $POD_NAME -- bash -c "apt-get update && apt-get install -y curl netcat && curl http://kubernetes.default"
```

---

## Lab 3: Troubleshoot Image Pull Errors

### Overview
Image pull errors are common in container deployments. Learn to diagnose and fix them.

### Common Image Pull Error Patterns

```bash
# Error 1: ImagePullBackOff
# Symptoms: Pod shows ImagePullBackOff status
kubectl get pods

# NAME                    READY   STATUS             RESTARTS   AGE
# myapp-5d4b8c9f-x7k2m   0/1     ImagePullBackOff   0          2m

# Diagnose:
kubectl describe pod myapp-5d4b8c9f-x7k2m | grep -A 10 Events

# Common output:
# Failed to pull image "myacr.azurecr.io/myapp:v1.0": 
# rpc error: code = Unknown desc = Error response from daemon: 
# pull access denied for myacr.azurecr.io/myapp, repository does not exist 
# or may require 'docker login'
```

### Step 1: Diagnose Image Pull Issues

```bash
# Test image pull locally
docker pull myacr.azurecr.io/myapp:v1.0

# Common errors and solutions:

# Error: "unauthorized: authentication required"
# Solution: Login to registry
az acr login --name myacr

# Error: "manifest unknown" or "not found"
# Solution: Check if image exists
az acr repository list --name myacr --output table
az acr repository show-tags --name myacr --repository myapp --output table

# Error: "denied: requested access to the resource is denied"
# Solution: Check permissions
az acr show --name myacr --resource-group $RESOURCE_GROUP --query "adminUserEnabled"

# Enable admin user if needed (not recommended for production)
az acr update --name myacr --resource-group $RESOURCE_GROUP --admin-enabled true
```

### Step 2: Fix Authentication Issues in AKS

```bash
# Solution 1: Attach ACR to AKS (Recommended)
az aks update \
  --resource-group $AKS_RG \
  --name $AKS_CLUSTER \
  --attach-acr $(az acr show --name myacr --resource-group $RESOURCE_GROUP --query id -o tsv)

# Verify integration
az aks check-acr \
  --resource-group $AKS_RG \
  --name $AKS_CLUSTER \
  --acr myacr.azurecr.io

# Solution 2: Create image pull secret (if ACR attach not possible)
# Get ACR credentials
export ACR_USERNAME=$(az acr credential show --name myacr --query "username" -o tsv)
export ACR_PASSWORD=$(az acr credential show --name myacr --query "passwords[0].value" -o tsv)

# Create Kubernetes secret
kubectl create secret docker-registry acr-secret \
  --docker-server=myacr.azurecr.io \
  --docker-username=$ACR_USERNAME \
  --docker-password=$ACR_PASSWORD \
  --docker-email=user@progresssoftware.com

# Verify secret
kubectl get secret acr-secret -o yaml

# Use secret in deployment
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      imagePullSecrets:
      - name: acr-secret
      containers:
      - name: myapp
        image: myacr.azurecr.io/myapp:v1.0
        ports:
        - containerPort: 80
EOF
```

### Step 3: Fix Network-Related Pull Issues

```bash
# Check if nodes can reach ACR
kubectl get nodes -o wide

export NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')

# Create debug pod on specific node
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: network-debug
spec:
  nodeName: $NODE_NAME
  containers:
  - name: debug
    image: nicolaka/netshoot
    command: ["sleep", "3600"]
EOF

# Test ACR connectivity from node
kubectl exec -it network-debug -- bash -c "nslookup myacr.azurecr.io"
kubectl exec -it network-debug -- bash -c "curl -I https://myacr.azurecr.io/v2/"

# Check firewall rules
az acr show --name myacr --resource-group $RESOURCE_GROUP --query "networkRuleSet"

# If public access is disabled, enable it temporarily for testing
az acr update --name myacr --resource-group $RESOURCE_GROUP --public-network-enabled true

# Or add AKS subnet to allowed networks
export VNET_ID=$(az aks show --resource-group $AKS_RG --name $AKS_CLUSTER --query "agentPoolProfiles[0].vnetSubnetId" -o tsv)

az acr network-rule add \
  --name myacr \
  --resource-group $RESOURCE_GROUP \
  --subnet $VNET_ID
```

### Step 4: Fix Image Tag Issues

```bash
# Issue: Using "latest" tag without proper updates
# Bad practice:
# image: myacr.azurecr.io/myapp:latest

# Solution: Use specific version tags
# image: myacr.azurecr.io/myapp:v1.0.0

# If you must use latest, ensure image pull policy is correct
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-latest
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myacr.azurecr.io/myapp:latest
        imagePullPolicy: Always  # Force pull on every pod creation
EOF

# Verify image pull policy
kubectl get deployment myapp-latest -o=jsonpath='{.spec.template.spec.containers[0].imagePullPolicy}'
```

---

## Lab 4: Diagnose Networking Issues

### Overview
Network troubleshooting is critical for containerized applications.

### Step 1: Service Discovery Issues

```bash
# Create test deployment and service
kubectl create deployment web --image=nginx
kubectl expose deployment web --port=80 --type=ClusterIP

# Test service resolution
kubectl run test-pod --image=busybox --rm -it -- /bin/sh

# Inside test pod:
nslookup web
nslookup web.default.svc.cluster.local
wget -O- http://web
exit

# If DNS fails, check CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns

kubectl logs -n kube-system -l k8s-app=kube-dns --tail=50

# Check CoreDNS configuration
kubectl get configmap -n kube-system coredns -o yaml

# Test DNS from node
kubectl run dnstest --image=busybox:1.28 --rm -it -- nslookup kubernetes.default
```

### Step 2: Pod-to-Pod Communication

```bash
# Create two deployments
kubectl create deployment backend --image=nginx
kubectl create deployment frontend --image=alpine --replicas=1 -- sleep 3600

# Expose backend
kubectl expose deployment backend --port=80 --name=backend-svc

# Get pod IPs
kubectl get pods -o wide

export BACKEND_POD=$(kubectl get pods -l app=backend -o jsonpath='{.items[0].metadata.name}')
export FRONTEND_POD=$(kubectl get pods -l app=frontend -o jsonpath='{.items[0].metadata.name}')

export BACKEND_IP=$(kubectl get pod $BACKEND_POD -o jsonpath='{.status.podIP}')

# Test pod-to-pod communication
kubectl exec -it $FRONTEND_POD -- sh -c "apk add curl && curl http://$BACKEND_IP"

# Test service communication
kubectl exec -it $FRONTEND_POD -- sh -c "curl http://backend-svc"

# If fails, check network policies
kubectl get networkpolicies --all-namespaces

# Check if network plugin is healthy (Azure CNI or kubenet)
kubectl get pods -n kube-system | grep -E "azure-cni|kube-proxy"
```

### Step 3: External Connectivity Issues

```bash
# Create LoadBalancer service
kubectl expose deployment web --port=80 --type=LoadBalancer --name=web-external

# Wait for external IP
kubectl get service web-external --watch

# Test external connectivity
export EXTERNAL_IP=$(kubectl get service web-external -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

curl http://$EXTERNAL_IP

# If timeout, check Azure Load Balancer
az network lb list --resource-group MC_${AKS_RG}_${AKS_CLUSTER}_${LOCATION} --output table

# Check health probes
az network lb probe list \
  --resource-group MC_${AKS_RG}_${AKS_CLUSTER}_${LOCATION} \
  --lb-name kubernetes \
  --output table

# Check NSG rules
az network nsg list \
  --resource-group MC_${AKS_RG}_${AKS_CLUSTER}_${LOCATION} \
  --output table

# Check if pod is ready
kubectl describe pod $BACKEND_POD | grep -A 5 "Readiness"

# Add health check to deployment
kubectl patch deployment web --type='json' -p='[
  {
    "op": "add",
    "path": "/spec/template/spec/containers/0/readinessProbe",
    "value": {
      "httpGet": {
        "path": "/",
        "port": 80
      },
      "initialDelaySeconds": 5,
      "periodSeconds": 10
    }
  }
]'
```

### Step 4: Ingress Issues

```bash
# Install nginx ingress controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml

# Wait for ingress controller
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=120s

# Create ingress resource
cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web
            port:
              number: 80
EOF

# Get ingress external IP
kubectl get ingress web-ingress

# Test ingress (with host header)
export INGRESS_IP=$(kubectl get service -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

curl -H "Host: myapp.example.com" http://$INGRESS_IP

# Check ingress controller logs
kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller --tail=100

# Debug ingress configuration
kubectl describe ingress web-ingress

# Check ingress controller configuration
kubectl exec -n ingress-nginx -it $(kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller -o jsonpath='{.items[0].metadata.name}') -- cat /etc/nginx/nginx.conf | grep -A 20 "server_name myapp.example.com"
```

---

## Lab 5: Resolve Resource Constraint Problems

### Overview
Resource constraints can cause performance issues, crashes, and scheduling failures.

### Step 1: Identify Resource-Starved Pods

```bash
# Check pod resource usage
kubectl top pods --all-namespaces --sort-by=memory
kubectl top pods --all-namespaces --sort-by=cpu

# Check node resource usage
kubectl top nodes

# Describe node to see allocatable resources
kubectl describe node $NODE_NAME

# Look for resource pressure conditions
kubectl describe node $NODE_NAME | grep -A 5 "Conditions:"

# Expected conditions:
# MemoryPressure   False
# DiskPressure     False
# PIDPressure      False
# Ready            True

# Find pods with resource issues
kubectl get pods --all-namespaces --field-selector=status.phase!=Running

# Check for OOMKilled pods
kubectl get pods --all-namespaces -o json | \
  jq '.items[] | select(.status.containerStatuses[]?.lastState.terminated.reason=="OOMKilled") | {name:.metadata.name, namespace:.metadata.namespace, reason:.status.containerStatuses[].lastState.terminated.reason}'
```

### Step 2: Fix Memory Issues

```bash
# Create deployment with inadequate memory
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: memory-constrained
spec:
  replicas: 1
  selector:
    matchLabels:
      app: memory-test
  template:
    metadata:
      labels:
        app: memory-test
    spec:
      containers:
      - name: app
        image: nginx
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "64Mi"  # Too restrictive
            cpu: "200m"
EOF

# Monitor for OOM kills
kubectl describe pod -l app=memory-test | grep -A 10 "Last State"

# If OOMKilled, increase limits
kubectl patch deployment memory-constrained --type='json' -p='[
  {
    "op": "replace",
    "path": "/spec/template/spec/containers/0/resources/limits/memory",
    "value": "256Mi"
  },
  {
    "op": "replace",
    "path": "/spec/template/spec/containers/0/resources/requests/memory",
    "value": "128Mi"
  }
]'

# Verify update
kubectl describe deployment memory-constrained | grep -A 10 "Limits"
```

### Step 3: Fix CPU Throttling

```bash
# Create CPU-intensive workload
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-intensive
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cpu-test
  template:
    metadata:
      labels:
        app: cpu-test
    spec:
      containers:
      - name: app
        image: ubuntu
        command: ["/bin/bash", "-c"]
        args:
          - |
            apt-get update && apt-get install -y stress
            stress --cpu 4 --timeout 600s
        resources:
          requests:
            cpu: "100m"
          limits:
            cpu: "500m"  # Will throttle
EOF

# Monitor CPU usage and throttling
kubectl top pod -l app=cpu-test

# Check throttling metrics using kubectl
export POD_NAME=$(kubectl get pods -l app=cpu-test -o jsonpath='{.items[0].metadata.name}')

# View container metrics from kubelet (requires node access)
kubectl get --raw /api/v1/nodes/$NODE_NAME/proxy/metrics | grep -A 5 "container_cpu_cfs_throttled_seconds_total.*$POD_NAME"

# Solution: Increase CPU limits
kubectl patch deployment cpu-intensive --type='json' -p='[
  {
    "op": "replace",
    "path": "/spec/template/spec/containers/0/resources/limits/cpu",
    "value": "2000m"
  }
]'

# Or remove CPU limits (allow bursting)
kubectl patch deployment cpu-intensive --type='json' -p='[
  {
    "op": "remove",
    "path": "/spec/template/spec/containers/0/resources/limits/cpu"
  }
]'
```

### Step 4: Fix Scheduling Issues

```bash
# Create pod that won't schedule due to resource constraints
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: large-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "100Gi"  # Too large for any node
        cpu: "32"
EOF

# Check pod status
kubectl get pod large-pod

# Expected: Pending

# Describe to see why
kubectl describe pod large-pod | grep -A 10 "Events:"

# Expected output:
# Warning  FailedScheduling  pod/large-pod  0/3 nodes are available: 
# 3 Insufficient memory, 3 Insufficient cpu.

# Solutions:
# 1. Reduce resource requests
kubectl delete pod large-pod

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: large-pod-fixed
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
EOF

# 2. Or scale up cluster
az aks scale \
  --resource-group $AKS_RG \
  --name $AKS_CLUSTER \
  --node-count 5

# 3. Or add new node pool with larger VMs
az aks nodepool add \
  --resource-group $AKS_RG \
  --cluster-name $AKS_CLUSTER \
  --name largerpool \
  --node-count 1 \
  --node-vm-size Standard_D8s_v3

# Use node selectors to target specific pool
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: large-pod-targeted
spec:
  nodeSelector:
    agentpool: largerpool
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "16Gi"
        cpu: "4"
EOF
```

---

## Lab 6: Performance Optimization Techniques

### Overview
Optimize container performance for production workloads.

### Step 1: Optimize Container Images

```bash
# Bad: Large base image
cat > Dockerfile.unoptimized <<'EOF'
FROM ubuntu:latest
RUN apt-get update && apt-get install -y \
    dotnet-sdk-8.0 \
    git \
    vim \
    curl \
    wget \
    build-essential
COPY . /app
WORKDIR /app
RUN dotnet publish -c Release -o out
CMD ["dotnet", "out/MyApp.dll"]
EOF

# Build and check size
docker build -f Dockerfile.unoptimized -t myapp:unoptimized .
docker images myapp:unoptimized

# Expected: ~2GB or larger

# Good: Optimized multi-stage build
cat > Dockerfile.optimized <<'EOF'
FROM mcr.microsoft.com/dotnet/sdk:8.0-alpine AS build
WORKDIR /src
COPY ["MyApp.csproj", "./"]
RUN dotnet restore
COPY . .
RUN dotnet publish -c Release -o /app/publish \
    -r linux-musl-x64 \
    --self-contained false \
    /p:PublishTrimmed=false \
    /p:PublishSingleFile=false

FROM mcr.microsoft.com/dotnet/aspnet:8.0-alpine
WORKDIR /app
COPY --from=build /app/publish .

# Run as non-root user
RUN addgroup -g 1000 appgroup && \
    adduser -u 1000 -G appgroup -s /bin/sh -D appuser && \
    chown -R appuser:appgroup /app

USER appuser

ENTRYPOINT ["dotnet", "MyApp.dll"]
EOF

# Build and compare size
docker build -f Dockerfile.optimized -t myapp:optimized .
docker images myapp:optimized

# Expected: ~200MB (10x smaller!)

# Use .dockerignore to exclude unnecessary files
cat > .dockerignore <<'EOF'
bin/
obj/
.git/
.vs/
*.md
Dockerfile*
.dockerignore
*.log
*.tmp
node_modules/
EOF
```

### Step 2: Enable Horizontal Pod Autoscaling (HPA)

```bash
# Create deployment
kubectl create deployment web-scalable --image=nginx --replicas=2

# Set resource requests (required for HPA)
kubectl set resources deployment web-scalable --limits=cpu=500m,memory=512Mi --requests=cpu=200m,memory=256Mi

# Install metrics-server if not present
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Wait for metrics-server
kubectl wait --for=condition=ready pod -n kube-system -l k8s-app=metrics-server --timeout=120s

# Create HPA
kubectl autoscale deployment web-scalable \
  --cpu-percent=70 \
  --min=2 \
  --max=10

# Check HPA status
kubectl get hpa web-scalable

# Generate load to trigger scaling
kubectl run load-generator --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://web-scalable; done" &

# Watch HPA scale the deployment
kubectl get hpa web-scalable --watch

# Clean up load generator
kubectl delete pod load-generator
```

### Step 3: Implement Readiness and Liveness Probes

```bash
# Deploy with health checks
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-with-probes
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: nginx
        ports:
        - containerPort: 80
        # Liveness: Check if container is alive
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 3
        # Readiness: Check if container is ready to serve traffic
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 2
          successThreshold: 1
          failureThreshold: 3
        # Startup: Give app time to start
        startupProbe:
          httpGet:
            path: /
            port: 80
          failureThreshold: 30
          periodSeconds: 10
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
EOF

# Monitor probe status
kubectl describe deployment app-with-probes | grep -A 20 "Liveness\|Readiness\|Startup"

# Simulate probe failure
export POD_NAME=$(kubectl get pods -l app=myapp -o jsonpath='{.items[0].metadata.name}')

kubectl exec -it $POD_NAME -- bash -c "rm /usr/share/nginx/html/index.html"

# Watch pod restart due to failed probe
kubectl get pods -l app=myapp --watch
```

### Step 4: Optimize Network Performance

```bash
# Use ClusterIP for internal services (fastest)
kubectl expose deployment app-with-probes --port=80 --type=ClusterIP --name=app-internal

# Use headless service for stateful apps
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: app-headless
spec:
  clusterIP: None  # Headless
  selector:
    app: myapp
  ports:
  - port: 80
    targetPort: 80
EOF

# Enable keep-alive in applications
# In .NET, add to Program.cs:
# builder.Services.Configure<KestrelServerOptions>(options =>
# {
#     options.AddServerHeader = false;
#     options.Limits.KeepAliveTimeout = TimeSpan.FromMinutes(2);
#     options.Limits.RequestHeadersTimeout = TimeSpan.FromMinutes(1);
# });

# Use connection pooling for databases
# In .NET connection string:
# "Server=mydb;Database=mydb;User Id=user;Password=pass;Pooling=true;Min Pool Size=5;Max Pool Size=100;"
```

### Step 5: Database Connection Optimization

```bash
# Example: Optimize .NET application for database performance

cat > OptimizedDbContext.cs <<'EOF'
using Microsoft.EntityFrameworkCore;
using Microsoft.Extensions.DependencyInjection;

public static class DatabaseExtensions
{
    public static IServiceCollection AddOptimizedDatabase(
        this IServiceCollection services, 
        string connectionString)
    {
        services.AddDbContext<AppDbContext>(options =>
        {
            options.UseSqlServer(connectionString, sqlOptions =>
            {
                // Enable connection resiliency
                sqlOptions.EnableRetryOnFailure(
                    maxRetryCount: 5,
                    maxRetryDelay: TimeSpan.FromSeconds(30),
                    errorNumbersToAdd: null);
                
                // Command timeout
                sqlOptions.CommandTimeout(30);
                
                // Use compiled models for performance
                sqlOptions.UseQuerySplittingBehavior(QuerySplittingBehavior.SplitQuery);
            });
            
            // Enable query caching
            options.EnableSensitiveDataLogging(false);
            options.EnableDetailedErrors(false);
            
            // Use query tracking behavior
            options.UseQueryTrackingBehavior(QueryTrackingBehavior.NoTracking);
        });
        
        // Add connection pooling
        services.AddDbContextPool<AppDbContext>(options => 
        {
            options.UseSqlServer(connectionString);
        }, poolSize: 128);
        
        return services;
    }
}
EOF
```

---

## Container Log Analysis Best Practices

### Structured Logging

```csharp
// Use structured logging in .NET applications
using Serilog;
using Serilog.Formatting.Json;

public class Program
{
    public static void Main(string[] args)
    {
        Log.Logger = new LoggerConfiguration()
            .MinimumLevel.Information()
            .Enrich.FromLogContext()
            .Enrich.WithProperty("Application", "MyApp")
            .Enrich.WithProperty("Environment", Environment.GetEnvironmentVariable("ASPNETCORE_ENVIRONMENT"))
            .WriteTo.Console(new JsonFormatter())
            .CreateLogger();
        
        try
        {
            Log.Information("Application starting...");
            CreateHostBuilder(args).Build().Run();
        }
        catch (Exception ex)
        {
            Log.Fatal(ex, "Application failed to start");
        }
        finally
        {
            Log.CloseAndFlush();
        }
    }
}
```

### Log Aggregation KQL Queries

```kql
-- Find all errors in the last hour
ContainerLog
| where TimeGenerated > ago(1h)
| where LogEntry has_any ("error", "exception", "fail", "fatal")
| extend LogLevel = extract(@"(ERROR|FATAL|EXCEPTION)", 1, LogEntry)
| project TimeGenerated, Computer, ContainerID, LogLevel, LogEntry
| order by TimeGenerated desc

-- Performance analysis from logs
ContainerLog
| where TimeGenerated > ago(1h)
| where LogEntry contains "duration"
| extend Duration = extract(@"duration[:\s]+([0-9]+)", 1, LogEntry, typeof(long))
| summarize 
    AvgDuration = avg(Duration),
    P95Duration = percentile(Duration, 95),
    MaxDuration = max(Duration)
    by bin(TimeGenerated, 5m)
| render timechart
```

---

## Using kubectl for AKS Troubleshooting

### Essential kubectl Commands Reference

```bash
# Cluster Information
kubectl cluster-info
kubectl get nodes -o wide
kubectl describe node <node-name>

# Pod Debugging
kubectl get pods --all-namespaces -o wide
kubectl describe pod <pod-name>
kubectl logs <pod-name> --tail=100 -f
kubectl logs <pod-name> --previous  # Previous container logs
kubectl exec -it <pod-name> -- /bin/bash

# Multi-container pods
kubectl logs <pod-name> -c <container-name>
kubectl exec -it <pod-name> -c <container-name> -- /bin/bash

# Resource Usage
kubectl top nodes
kubectl top pods --all-namespaces
kubectl top pod <pod-name> --containers

# Events
kubectl get events --sort-by='.lastTimestamp'
kubectl get events --field-selector involvedObject.name=<pod-name>

# Configuration
kubectl get configmap -o yaml
kubectl get secret -o yaml
kubectl describe service <service-name>

# Debugging Services
kubectl get endpoints
kubectl port-forward svc/<service-name> 8080:80
kubectl proxy

# Rollout Management
kubectl rollout status deployment/<deployment-name>
kubectl rollout history deployment/<deployment-name>
kubectl rollout undo deployment/<deployment-name>

# Resource Quotas and Limits
kubectl describe resourcequota
kubectl describe limitrange

# Network Policies
kubectl get networkpolicies
kubectl describe networkpolicy <policy-name>
```

---

## Common Error Messages and Solutions

### Error Reference Table

| Error Message | Cause | Solution |
|---------------|-------|----------|
| `CrashLoopBackOff` | Container crashes immediately after start | Check logs with `kubectl logs --previous`, fix application code or configuration |
| `ImagePullBackOff` | Can't pull container image | Verify image exists, check credentials, verify network connectivity |
| `CreateContainerConfigError` | ConfigMap or Secret missing | Verify ConfigMap/Secret exists: `kubectl get configmap/secret` |
| `OOMKilled` | Container exceeded memory limit | Increase memory limits or fix memory leak |
| `Pending` | Pod can't be scheduled | Check resource availability, node selectors, taints/tolerations |
| `ErrImagePull` | Image pull failed | Check registry authentication, verify image name and tag |
| `RunContainerError` | Error starting container | Check container command/args, verify volume mounts |
| `InvalidImageName` | Malformed image name | Fix image name format: `registry/repository:tag` |
| `Failed to pull image "…": rpc error: code = Unknown desc = Error response from daemon: Get https://…: unauthorized` | Registry authentication failed | Run `az acr login` or create image pull secret |
| `0/3 nodes available: insufficient cpu` | Not enough CPU resources | Scale cluster or reduce CPU requests |

---

## Summary

In this lab, you learned how to:
- ✅ Debug containers locally with Docker tools
- ✅ Troubleshoot remote containers in Azure services
- ✅ Diagnose and fix image pull errors
- ✅ Resolve networking and connectivity issues
- ✅ Fix resource constraint problems
- ✅ Optimize container performance
- ✅ Use kubectl effectively for AKS troubleshooting
- ✅ Analyze logs for root cause analysis

---

## Additional Resources

- [Docker Debugging Documentation](https://docs.docker.com/config/containers/debugging/)
- [Kubernetes Debugging Pods](https://kubernetes.io/docs/tasks/debug/debug-application/)
- [AKS Troubleshooting Guide](https://docs.microsoft.com/azure/aks/troubleshooting)
- [kubectl Cheat Sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/)
- [Container Insights Troubleshooting](https://docs.microsoft.com/azure/azure-monitor/containers/container-insights-troubleshoot)

---

## Next Steps

Continue to [Module 5.3: Production Best Practices](5.3-production-best-practices.md) to learn about production readiness, security, and high availability.

---

**Progress Software - Building Resilient Container Solutions**
