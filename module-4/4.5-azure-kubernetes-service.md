# Module 4.5: Azure Kubernetes Service (AKS)

## Overview
Azure Kubernetes Service (AKS) provides a fully managed Kubernetes container orchestration service. This comprehensive lab covers Kubernetes fundamentals, AKS cluster provisioning, deploying .NET applications, configuring autoscaling, and using Helm for package management.

---

## Prerequisites
- Active Azure subscription
- Azure CLI installed (version 2.30.0 or later)
- kubectl installed
- helm installed (version 3.x)
- Bash shell environment
- Azure Container Registry with .NET container images
- Understanding of container concepts from previous modules

---

## Learning Objectives
By the end of this lab, you will be able to:
- Understand Kubernetes fundamentals (pods, services, deployments)
- Provision AKS clusters with ACR integration
- Deploy .NET applications using Kubernetes manifests
- Create Ingress controllers with NGINX
- Configure Horizontal Pod Autoscaler (HPA)
- Enable cluster autoscaler
- Package and deploy applications with Helm
- Monitor clusters with Azure Monitor

---

## 1. Kubernetes Fundamentals Overview

### 1.1 Core Kubernetes Concepts

**Pod**: Smallest deployable unit, contains one or more containers
**Deployment**: Manages replica sets and rolling updates
**Service**: Exposes pods internally or externally with load balancing
**Ingress**: HTTP/HTTPS routing to services
**ConfigMap**: Configuration data for applications
**Secret**: Sensitive configuration data
**Namespace**: Virtual clusters for resource isolation

### 1.2 AKS Architecture

```
┌───────────────────────────────────────────────────────────┐
│              Azure Kubernetes Service                     │
│                                                           │
│  ┌─────────────────────────────────────────────────────┐ │
│  │           Control Plane (Managed by Azure)          │ │
│  │  - API Server  - Scheduler  - Controller Manager   │ │
│  └─────────────────────────────────────────────────────┘ │
│                          ↓                                │
│  ┌─────────────────────────────────────────────────────┐ │
│  │                  Node Pool 1                        │ │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐         │ │
│  │  │  Node 1  │  │  Node 2  │  │  Node 3  │         │ │
│  │  │          │  │          │  │          │         │ │
│  │  │ ┌─────┐  │  │ ┌─────┐  │  │ ┌─────┐  │         │ │
│  │  │ │ Pod │  │  │ │ Pod │  │  │ │ Pod │  │         │ │
│  │  │ │ Pod │  │  │ │ Pod │  │  │ │ Pod │  │         │ │
│  │  │ └─────┘  │  │ └─────┘  │  │ └─────┘  │         │ │
│  │  └──────────┘  └──────────┘  └──────────┘         │ │
│  └─────────────────────────────────────────────────────┘ │
│                          ↑                                │
│                  ┌───────────────┐                        │
│                  │  Azure CNI    │                        │
│                  │  (Networking) │                        │
│                  └───────────────┘                        │
└───────────────────────────────────────────────────────────┘
              ↓                    ↓
    ┌─────────────────┐   ┌─────────────────┐
    │      ACR        │   │ Load Balancer   │
    │   (Images)      │   │  (External IP)  │
    └─────────────────┘   └─────────────────┘
```

---

## 2. Setup and Prerequisites

### Lab 2.1: Install Required Tools

**Step 1: Verify Azure CLI**
```bash
# Check Azure CLI version
az --version

# Ensure version is 2.30.0 or later
# Update if needed: https://docs.microsoft.com/cli/azure/install-azure-cli
```

**Step 2: Install kubectl**
```bash
# Install kubectl via Azure CLI
az aks install-cli

# Verify installation
kubectl version --client

# Expected output: Client Version: v1.28.x or later
```

**Step 3: Install Helm**
```bash
# macOS
brew install helm

# Linux
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# Verify installation
helm version

# Expected output: version.BuildInfo{Version:"v3.x.x"...}
```

---

### Lab 2.2: Setup Environment Variables

```bash
# Configure lab variables
export RESOURCE_GROUP="rg-aks-lab"
export LOCATION="eastus"
export AKS_CLUSTER="myaks-cluster"
export ACR_NAME="<your-acr-name>"
export ACR_LOGIN_SERVER="${ACR_NAME}.azurecr.io"
export NODE_COUNT=3
export VM_SIZE="Standard_D2s_v3"

# Verify variables
echo "Resource Group: $RESOURCE_GROUP"
echo "AKS Cluster: $AKS_CLUSTER"
echo "Location: $LOCATION"
echo "ACR: $ACR_LOGIN_SERVER"
```

**Step 2: Create Resource Group**
```bash
# Create resource group
az group create \
  --name $RESOURCE_GROUP \
  --location $LOCATION

# Verify creation
az group show --name $RESOURCE_GROUP --output table
```

---

## 3. Lab 1: Provision AKS Cluster

### Lab 3.1: Create AKS Cluster with ACR Integration

**Step 1: Create AKS Cluster**
```bash
# Create AKS cluster with ACR integration
az aks create \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --node-count $NODE_COUNT \
  --node-vm-size $VM_SIZE \
  --enable-managed-identity \
  --attach-acr $ACR_NAME \
  --network-plugin azure \
  --enable-addons monitoring \
  --generate-ssh-keys \
  --zones 1 2 3

# This takes 5-10 minutes
echo "Creating AKS cluster... This will take several minutes."
```

**Expected Output:**
```json
{
  "id": "/subscriptions/.../resourceGroups/rg-aks-lab/providers/Microsoft.ContainerService/managedClusters/myaks-cluster",
  "location": "eastus",
  "name": "myaks-cluster",
  "provisioningState": "Succeeded",
  "powerState": {
    "code": "Running"
  }
}
```

**Key Parameters Explained:**
- `--enable-managed-identity`: Uses Azure AD managed identity for authentication
- `--attach-acr`: Grants AKS permission to pull from ACR
- `--network-plugin azure`: Uses Azure CNI (vs. kubenet)
- `--enable-addons monitoring`: Enables Azure Monitor for containers
- `--zones 1 2 3`: Distributes nodes across availability zones

---

### Lab 3.2: Verify Cluster Creation

**Step 1: Check Cluster Status**
```bash
# Get cluster details
az aks show \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --output table

# Verify ACR integration
az aks check-acr \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --acr $ACR_NAME
```

**Expected Output:**
```
Your cluster can pull images from myuniqueregistry.azurecr.io!
```

---

## 4. Lab 2: Connect to AKS Cluster

### Lab 4.1: Get Cluster Credentials

**Step 1: Download Kubeconfig**
```bash
# Get AKS credentials (updates ~/.kube/config)
az aks get-credentials \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --overwrite-existing

echo "Kubeconfig downloaded successfully"
```

**Step 2: Verify Connection**
```bash
# Test connection to cluster
kubectl get nodes

# Expected output: List of 3 nodes in Ready state
```

**Expected Output:**
```
NAME                                STATUS   ROLES   AGE     VERSION
aks-nodepool1-12345678-vmss000000   Ready    agent   5m30s   v1.28.5
aks-nodepool1-12345678-vmss000001   Ready    agent   5m25s   v1.28.5
aks-nodepool1-12345678-vmss000002   Ready    agent   5m28s   v1.28.5
```

**Step 3: Explore Cluster**
```bash
# Get cluster info
kubectl cluster-info

# List all namespaces
kubectl get namespaces

# Check default namespace pods
kubectl get pods --all-namespaces
```

---

## 5. Lab 3: Deploy .NET Application with Kubernetes Manifests

### Lab 5.1: Create Deployment Manifest

**Step 1: Create Working Directory**
```bash
# Create directory for Kubernetes manifests
mkdir -p ~/aks-demo/manifests
cd ~/aks-demo/manifests
```

**Step 2: Create Deployment YAML**
```bash
# Create deployment.yaml
cat > deployment.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mywebapi
  namespace: default
  labels:
    app: mywebapi
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mywebapi
  template:
    metadata:
      labels:
        app: mywebapi
    spec:
      containers:
      - name: mywebapi
        image: $ACR_LOGIN_SERVER/mywebapi:1.0
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: ASPNETCORE_ENVIRONMENT
          value: "Production"
        - name: LOG_LEVEL
          value: "Information"
        resources:
          requests:
            cpu: 250m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
EOF

echo "deployment.yaml created"
```

---

### Lab 5.2: Create Service Manifest

**Step 1: Create Service YAML**
```bash
# Create service.yaml
cat > service.yaml <<EOF
apiVersion: v1
kind: Service
metadata:
  name: mywebapi-service
  namespace: default
  labels:
    app: mywebapi
spec:
  type: LoadBalancer
  selector:
    app: mywebapi
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
    name: http
  sessionAffinity: None
EOF

echo "service.yaml created"
```

**Service Types Explained:**
- `ClusterIP` (default): Internal cluster IP only
- `NodePort`: Exposes on each node's IP at a static port
- `LoadBalancer`: Creates Azure Load Balancer with public IP

---

### Lab 5.3: Deploy Application

**Step 1: Apply Manifests**
```bash
# Apply deployment
kubectl apply -f deployment.yaml

# Apply service
kubectl apply -f service.yaml

# Wait for resources to be created
sleep 10
```

**Expected Output:**
```
deployment.apps/mywebapi created
service/mywebapi-service created
```

**Step 2: Verify Deployment**
```bash
# Check deployment status
kubectl get deployment mywebapi

# Expected: 3/3 replicas ready
```

**Expected Output:**
```
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
mywebapi   3/3     3            3           30s
```

**Step 3: Check Pods**
```bash
# List pods
kubectl get pods -l app=mywebapi

# Get detailed pod info
kubectl get pods -l app=mywebapi -o wide
```

**Expected Output:**
```
NAME                        READY   STATUS    RESTARTS   AGE   NODE
mywebapi-7d4b8c9f8d-2kqrt   1/1     Running   0          45s   aks-nodepool1-...-vmss000001
mywebapi-7d4b8c9f8d-7xmzn   1/1     Running   0          45s   aks-nodepool1-...-vmss000002
mywebapi-7d4b8c9f8d-qw9ks   1/1     Running   0          45s   aks-nodepool1-...-vmss000000
```

---

### Lab 5.4: Access the Application

**Step 1: Get External IP**
```bash
# Get service external IP (may take 1-2 minutes)
kubectl get service mywebapi-service --watch

# Press Ctrl+C when EXTERNAL-IP is assigned (not <pending>)
```

**Expected Output:**
```
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
mywebapi-service   LoadBalancer   10.0.142.23    20.102.45.123    80:31234/TCP   2m
```

**Step 2: Store External IP**
```bash
# Get external IP
export EXTERNAL_IP=$(kubectl get service mywebapi-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

echo "Application URL: http://$EXTERNAL_IP"
```

**Step 3: Test Application**
```bash
# Test health endpoint
curl http://$EXTERNAL_IP/health

# Test API endpoint
curl http://$EXTERNAL_IP/api/products
```

**Expected Response:**
```json
{"status":"Healthy","version":"1.0"}
```

---

### Lab 5.5: View Application Logs

**Step 1: View Logs from Specific Pod**
```bash
# Get pod name
export POD_NAME=$(kubectl get pods -l app=mywebapi -o jsonpath='{.items[0].metadata.name}')

# View logs
kubectl logs $POD_NAME

# Follow logs in real-time
kubectl logs $POD_NAME --follow

# Press Ctrl+C to stop
```

**Step 2: View Logs from All Pods**
```bash
# View logs from all pods with label
kubectl logs -l app=mywebapi --tail=50

# Stream logs from all pods
kubectl logs -l app=mywebapi --follow --max-log-requests=10
```

---

## 6. Lab 4: Create Ingress with NGINX Controller

### Lab 6.1: Install NGINX Ingress Controller

**Step 1: Create Ingress Namespace**
```bash
# Create namespace for ingress controller
kubectl create namespace ingress-nginx
```

**Step 2: Add Helm Repository**
```bash
# Add NGINX ingress Helm repo
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

# Update repo
helm repo update

# Verify repo
helm search repo ingress-nginx
```

**Step 3: Install NGINX Ingress Controller**
```bash
# Install ingress controller with Helm
helm install ingress-nginx ingress-nginx/ingress-nginx \
  --namespace ingress-nginx \
  --set controller.service.annotations."service\.beta\.kubernetes\.io/azure-load-balancer-health-probe-request-path"=/healthz \
  --set controller.service.externalTrafficPolicy=Local

# Wait for installation
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=120s
```

**Expected Output:**
```
pod/ingress-nginx-controller-xxxxx condition met
```

**Step 4: Verify Ingress Controller**
```bash
# Check ingress controller pod
kubectl get pods -n ingress-nginx

# Get ingress controller service
kubectl get service -n ingress-nginx
```

**Expected Output:**
```
NAME                                 TYPE           EXTERNAL-IP      PORT(S)
ingress-nginx-controller             LoadBalancer   20.102.45.200    80:30080/TCP,443:30443/TCP
```

---

### Lab 6.2: Create Ingress Resource

**Step 1: Get Ingress Controller IP**
```bash
# Get ingress external IP
export INGRESS_IP=$(kubectl get service ingress-nginx-controller \
  -n ingress-nginx \
  -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

echo "Ingress IP: $INGRESS_IP"
```

**Step 2: Update Service to ClusterIP**
```bash
# Change service type from LoadBalancer to ClusterIP
kubectl patch service mywebapi-service \
  -p '{"spec":{"type":"ClusterIP"}}'

# Verify change
kubectl get service mywebapi-service
```

**Step 3: Create Ingress Manifest**
```bash
# Create ingress.yaml
cat > ingress.yaml <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mywebapi-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  rules:
  - host: mywebapi.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mywebapi-service
            port:
              number: 80
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: mywebapi-service
            port:
              number: 80
EOF

echo "ingress.yaml created"
```

**Step 4: Apply Ingress**
```bash
# Apply ingress
kubectl apply -f ingress.yaml

# Wait a moment for configuration
sleep 10

# Verify ingress
kubectl get ingress mywebapi-ingress
```

**Expected Output:**
```
NAME                CLASS   HOSTS              ADDRESS         PORTS   AGE
mywebapi-ingress    nginx   mywebapi.local     20.102.45.200   80      30s
```

**Step 5: Test Ingress**
```bash
# Test with Host header
curl http://$INGRESS_IP/health -H "Host: mywebapi.local"

# Test without host (using wildcard rule)
curl http://$INGRESS_IP/health

# Expected response: {"status":"Healthy"}
```

---

## 7. Lab 5: Configure Horizontal Pod Autoscaler (HPA)

### Lab 7.1: Install Metrics Server (if needed)

**Step 1: Check Metrics Server**
```bash
# Check if metrics server is running
kubectl get deployment metrics-server -n kube-system

# If not found, install it
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Wait for metrics server
kubectl wait --for=condition=available --timeout=120s deployment/metrics-server -n kube-system
```

**Step 2: Verify Metrics**
```bash
# Check node metrics
kubectl top nodes

# Check pod metrics
kubectl top pods -l app=mywebapi
```

**Expected Output:**
```
NAME                        CPU(cores)   MEMORY(bytes)
mywebapi-7d4b8c9f8d-2kqrt   15m          128Mi
mywebapi-7d4b8c9f8d-7xmzn   12m          125Mi
mywebapi-7d4b8c9f8d-qw9ks   18m          135Mi
```

---

### Lab 7.2: Create Horizontal Pod Autoscaler

**Step 1: Create HPA**
```bash
# Create HPA based on CPU utilization
kubectl autoscale deployment mywebapi \
  --cpu-percent=70 \
  --min=2 \
  --max=20

echo "HPA created: Scale when CPU > 70%, min 2, max 20 pods"
```

**Step 2: View HPA Status**
```bash
# Get HPA status
kubectl get hpa mywebapi

# Watch HPA (updates every 15 seconds)
kubectl get hpa mywebapi --watch
```

**Expected Output:**
```
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
mywebapi   Deployment/mywebapi   15%/70%   2         20        3          30s
```

**Step 3: Describe HPA**
```bash
# Get detailed HPA information
kubectl describe hpa mywebapi
```

---

### Lab 7.3: Test HPA with Load

**Step 1: Generate Load**
```bash
# Run load generator in a pod
kubectl run load-generator \
  --image=busybox \
  --restart=Never \
  -- /bin/sh -c "while true; do wget -q -O- http://mywebapi-service/api/compute; done"

# Or use multiple load generators
for i in {1..5}; do
  kubectl run load-generator-$i \
    --image=busybox \
    --restart=Never \
    -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://mywebapi-service/api/compute; done" &
done
```

**Step 2: Monitor Scaling**
```bash
# Watch HPA scale out
kubectl get hpa mywebapi --watch

# Watch pod count increase
watch -n 5 'kubectl get pods -l app=mywebapi'

# Monitor pod CPU usage
watch -n 5 'kubectl top pods -l app=mywebapi'
```

**Expected Behavior:**
- CPU usage increases above 70%
- HPA scales up replicas gradually
- Pods distribute across nodes

**Step 3: Stop Load and Watch Scale In**
```bash
# Delete load generators
kubectl delete pod load-generator
kubectl delete pod load-generator-{1..5}

# Watch HPA scale in (takes 5-10 minutes)
kubectl get hpa mywebapi --watch

# HPA will scale down to min replicas (2) after cooldown period
```

---

## 8. Lab 6: Enable Cluster Autoscaler

### Lab 8.1: Enable Cluster Autoscaler

**Step 1: Update Node Pool with Autoscaling**
```bash
# Enable cluster autoscaler
az aks update \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --enable-cluster-autoscaler \
  --min-count 1 \
  --max-count 10

echo "Cluster autoscaler enabled: 1-10 nodes"
```

**Step 2: Verify Cluster Autoscaler**
```bash
# Check autoscaler configuration
az aks show \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --query agentPoolProfiles[0].{MinCount:minCount,MaxCount:maxCount,EnableAutoScaling:enableAutoScaling}
```

**Expected Output:**
```json
{
  "EnableAutoScaling": true,
  "MaxCount": 10,
  "MinCount": 1
}
```

---

### Lab 8.2: Test Cluster Autoscaler

**Step 1: Create Resource-Intensive Deployment**
```bash
# Create deployment that requires more resources
cat > stress-deployment.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stress-app
spec:
  replicas: 20
  selector:
    matchLabels:
      app: stress-app
  template:
    metadata:
      labels:
        app: stress-app
    spec:
      containers:
      - name: stress
        image: polinux/stress
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        command: ["stress"]
        args: ["--cpu", "2", "--timeout", "600s"]
EOF

# Apply deployment
kubectl apply -f stress-deployment.yaml
```

**Step 2: Monitor Cluster Scaling**
```bash
# Watch pods pending due to insufficient resources
watch -n 5 'kubectl get pods -l app=stress-app | grep Pending | wc -l'

# Watch nodes being added
watch -n 10 'kubectl get nodes'

# Check cluster autoscaler logs
kubectl logs -n kube-system -l app=cluster-autoscaler --tail=50
```

**Expected Behavior:**
- Some pods remain in "Pending" state
- Cluster autoscaler provisions new nodes
- New nodes become "Ready"
- Pending pods get scheduled

**Step 3: Clean Up and Watch Scale Down**
```bash
# Delete stress deployment
kubectl delete -f stress-deployment.yaml

# Watch nodes scale down (takes 10-15 minutes)
watch -n 30 'kubectl get nodes'

# Cluster autoscaler removes underutilized nodes
```

---

## 9. Lab 7: Package and Deploy with Helm

### Lab 9.1: Create Helm Chart

**Step 1: Create Helm Chart Structure**
```bash
# Create new Helm chart
helm create mywebapi-chart

# Examine chart structure
ls -la mywebapi-chart/

# Structure:
# mywebapi-chart/
#   Chart.yaml          # Chart metadata
#   values.yaml         # Default values
#   templates/          # Kubernetes manifests
#     deployment.yaml
#     service.yaml
#     ingress.yaml
#     ...
```

**Step 2: Customize Chart Metadata**
```bash
# Edit Chart.yaml
cat > mywebapi-chart/Chart.yaml <<EOF
apiVersion: v2
name: mywebapi-chart
description: A Helm chart for .NET Web API on AKS
type: application
version: 1.0.0
appVersion: "1.0"
maintainers:
  - name: Progress Software
    email: devops@progress.com
EOF
```

**Step 3: Customize Values**
```bash
# Edit values.yaml
cat > mywebapi-chart/values.yaml <<EOF
replicaCount: 3

image:
  repository: $ACR_LOGIN_SERVER/mywebapi
  tag: "1.0"
  pullPolicy: IfNotPresent

service:
  type: LoadBalancer
  port: 80
  targetPort: 8080

ingress:
  enabled: false
  className: nginx
  hosts:
    - host: mywebapi.local
      paths:
        - path: /
          pathType: Prefix

resources:
  requests:
    cpu: 250m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 1Gi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70

env:
  - name: ASPNETCORE_ENVIRONMENT
    value: "Production"
  - name: LOG_LEVEL
    value: "Information"

livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /health/ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
EOF
```

---

### Lab 9.2: Deploy with Helm

**Step 1: Validate Chart**
```bash
# Lint the chart
helm lint mywebapi-chart

# Dry-run to see generated manifests
helm install mywebapi mywebapi-chart --dry-run --debug
```

**Step 2: Install Chart**
```bash
# Install the Helm chart
helm install mywebapi mywebapi-chart \
  --namespace default \
  --create-namespace

echo "Helm chart installed"
```

**Expected Output:**
```
NAME: mywebapi
LAST DEPLOYED: Wed Jan 29 10:30:00 2026
NAMESPACE: default
STATUS: deployed
REVISION: 1
```

**Step 3: Verify Installation**
```bash
# List Helm releases
helm list

# Get release status
helm status mywebapi

# Check deployed resources
kubectl get all -l app.kubernetes.io/instance=mywebapi
```

---

### Lab 9.3: Update Release

**Step 1: Modify Values**
```bash
# Update replica count
helm upgrade mywebapi mywebapi-chart \
  --set replicaCount=5 \
  --set image.tag=2.0

echo "Release upgraded to 5 replicas with image tag 2.0"
```

**Step 2: View Release History**
```bash
# List release revisions
helm history mywebapi
```

**Expected Output:**
```
REVISION  UPDATED                   STATUS      CHART                APP VERSION
1         Wed Jan 29 10:30:00 2026  superseded  mywebapi-chart-1.0.0 1.0
2         Wed Jan 29 10:35:00 2026  deployed    mywebapi-chart-1.0.0 1.0
```

**Step 3: Rollback if Needed**
```bash
# Rollback to previous revision
helm rollback mywebapi 1

# Verify rollback
helm history mywebapi
kubectl get pods -l app.kubernetes.io/instance=mywebapi
```

---

### Lab 9.4: Package and Share Chart

**Step 1: Package Chart**
```bash
# Package the chart
helm package mywebapi-chart

# Expected output: mywebapi-chart-1.0.0.tgz
```

**Step 2: Install from Package**
```bash
# Install from packaged chart
helm install mywebapi-v2 ./mywebapi-chart-1.0.0.tgz \
  --namespace demo \
  --create-namespace
```

---

## 10. Monitoring with Azure Monitor

### Lab 10.1: View Container Insights

**Step 1: Access Azure Portal**
```bash
# Open Azure Portal for cluster
az aks browse \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER

# Or get portal URL
echo "Azure Portal: https://portal.azure.com/#resource$(az aks show -g $RESOURCE_GROUP -n $AKS_CLUSTER --query id -o tsv)"
```

**Step 2: Navigate to Insights**
1. Go to Azure Portal
2. Find your AKS cluster
3. Click "Insights" under Monitoring
4. Explore:
   - Cluster view
   - Nodes view
   - Controllers view
   - Containers view

---

### Lab 10.2: Query Logs with Log Analytics

**Step 1: Get Log Analytics Workspace**
```bash
# Get workspace ID
export WORKSPACE_ID=$(az aks show \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --query addonProfiles.omsagent.config.logAnalyticsWorkspaceResourceID \
  --output tsv)

echo "Log Analytics Workspace: $WORKSPACE_ID"
```

**Step 2: Sample Kusto Queries**

```kusto
// Query 1: Container logs
ContainerLog
| where ClusterName == "myaks-cluster"
| where ContainerName contains "mywebapi"
| order by TimeGenerated desc
| take 100

// Query 2: Pod inventory
KubePodInventory
| where ClusterName == "myaks-cluster"
| where Namespace == "default"
| summarize count() by Name, PodStatus
| render barchart

// Query 3: Node CPU usage
Perf
| where ObjectName == "K8SNode"
| where CounterName == "cpuUsageNanoCores"
| summarize AvgCPU = avg(CounterValue) by Computer, bin(TimeGenerated, 5m)
| render timechart

// Query 4: Container restarts
KubePodInventory
| where ClusterName == "myaks-cluster"
| where RestartCount > 0
| summarize TotalRestarts = sum(RestartCount) by ContainerName, Namespace
| order by TotalRestarts desc
```

---

## 11. Best Practices

### 11.1 Security Best Practices

✅ **DO:**
- Use managed identities for ACR integration
- Enable RBAC and Azure AD integration
- Use network policies to restrict pod communication
- Scan container images for vulnerabilities
- Rotate credentials regularly
- Use Azure Policy for governance
- Enable pod security policies
- Use secrets for sensitive data (not ConfigMaps)

❌ **DON'T:**
- Run containers as root
- Use admin credentials for ACR
- Expose cluster without network policies
- Store secrets in environment variables
- Skip security scanning of images

---

### 11.2 Resource Management Best Practices

**Resource Requests and Limits:**
```yaml
resources:
  requests:     # Minimum resources guaranteed
    cpu: 250m
    memory: 512Mi
  limits:       # Maximum resources allowed
    cpu: 500m
    memory: 1Gi
```

**Best Practices:**
- Always set resource requests and limits
- Set requests based on average usage
- Set limits 1.5-2x higher than requests
- Use namespaces for resource isolation
- Implement resource quotas per namespace

---

### 11.3 High Availability Best Practices

✅ **DO:**
- Use multiple replicas (minimum 3 for production)
- Distribute across availability zones
- Implement health checks (liveness and readiness)
- Use pod disruption budgets
- Configure HPA for automatic scaling
- Use topology spread constraints

**Pod Disruption Budget Example:**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mywebapi-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: mywebapi
```

---

## 12. Troubleshooting Guide

### Issue 1: Pods Not Starting

**Solutions:**
```bash
# Check pod status
kubectl get pods -l app=mywebapi

# Describe pod for events
kubectl describe pod <pod-name>

# Check logs
kubectl logs <pod-name>

# Common issues:
# 1. Image pull errors (check ACR integration)
# 2. Resource constraints (insufficient CPU/memory)
# 3. Failed health checks
# 4. Missing dependencies (ConfigMaps, Secrets)
```

### Issue 2: Service Not Accessible

**Solutions:**
```bash
# Check service endpoints
kubectl get endpoints mywebapi-service

# Verify service configuration
kubectl describe service mywebapi-service

# Test from within cluster
kubectl run test --image=busybox --rm -it --restart=Never -- wget -O- http://mywebapi-service

# Common issues:
# 1. Label mismatch between service and pods
# 2. Wrong target port
# 3. Network policy blocking traffic
# 4. Load balancer not provisioned (pending external IP)
```

### Issue 3: HPA Not Scaling

**Solutions:**
```bash
# Check metrics server
kubectl top nodes
kubectl top pods

# Verify HPA configuration
kubectl describe hpa mywebapi

# Check resource requests are set
kubectl get deployment mywebapi -o yaml | grep -A 5 resources

# Common issues:
# 1. Metrics server not installed
# 2. No resource requests defined
# 3. Current usage below threshold
# 4. Insufficient cluster capacity
```

---

## 13. Cleanup

### Lab 13.1: Delete Resources

```bash
# Delete Helm releases
helm uninstall mywebapi

# Delete Kubernetes resources
kubectl delete -f deployment.yaml
kubectl delete -f service.yaml
kubectl delete -f ingress.yaml

# Delete namespace
kubectl delete namespace ingress-nginx

# Delete AKS cluster
az aks delete \
  --resource-group $RESOURCE_GROUP \
  --name $AKS_CLUSTER \
  --yes \
  --no-wait

# Delete resource group
az group delete \
  --name $RESOURCE_GROUP \
  --yes \
  --no-wait

echo "Cleanup initiated"
```

---

## Summary

You have successfully completed the comprehensive AKS lab and learned:

✅ Kubernetes fundamentals and architecture  
✅ Provisioning AKS clusters with ACR integration  
✅ Deploying .NET applications with Kubernetes manifests  
✅ Creating Ingress controllers with NGINX  
✅ Configuring Horizontal Pod Autoscaler (HPA)  
✅ Enabling cluster autoscaler for node scaling  
✅ Packaging and deploying applications with Helm  
✅ Monitoring clusters with Azure Monitor  
✅ Best practices for security, performance, and high availability  
✅ Troubleshooting common AKS issues  

**Next Steps:**
- Proceed to **Module 4.6** for CI/CD pipelines
- Explore service meshes (Istio, Linkerd)
- Implement GitOps with Flux or Argo CD
- Learn about AKS advanced networking

---

**© 2026 Progress Software Corporation. All rights reserved.**
